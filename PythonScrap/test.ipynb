{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入套件\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前正在搜尋股票: 2308台達電 在Google的新聞清單  進度: 1 / 1\n",
      "目前正在下載: 2308台達電 各家新聞  進度: 1 / 7\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\star8\\Desktop\\Scraping\\test.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39mfor\u001b[39;00m iLink \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m目前正在下載: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m searchList[iSearch] \u001b[39m+\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m           \u001b[39m'\u001b[39m\u001b[39m 各家新聞  進度: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(iLink \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m / \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mlen\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m])))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m     newsUrl, content \u001b[39m=\u001b[39m beautifulSoupNews(url\u001b[39m=\u001b[39;49mdf[\u001b[39m'\u001b[39;49m\u001b[39mlink\u001b[39;49m\u001b[39m'\u001b[39;49m][iLink])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m     newsUrls\u001b[39m.\u001b[39mappend(newsUrl)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     contents\u001b[39m.\u001b[39mappend(content)\n",
      "\u001b[1;32mc:\\Users\\star8\\Desktop\\Scraping\\test.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     content \u001b[39m=\u001b[39m content\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39melif\u001b[39;00m domain \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mwww.ctee.com.tw\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39m# 工商時報\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     item \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mfind_all(\u001b[39m'\u001b[39;49m\u001b[39mdiv\u001b[39;49m\u001b[39m'\u001b[39;49m, class_\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mentry-content clearfix single-post-content\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     content \u001b[39m=\u001b[39m [elem\u001b[39m.\u001b[39mgetText() \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m item]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/star8/Desktop/Scraping/test.ipynb#W1sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     content \u001b[39m=\u001b[39m [elem \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m content]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "searchList = ['2887台新金']\n",
    "\n",
    "# 新聞下載起始日\n",
    "nearStartDate = (datetime.date.today() + datetime.timedelta(days=-3000)).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# 整理Google新聞資料用\n",
    "def arrangeGoogleNews(elem):\n",
    "    return ([elem.find('title').getText(),\n",
    "             elem.find('link').getText(),\n",
    "             elem.find('pubDate').getText(),\n",
    "             BeautifulSoup(elem.find('description').getText(), 'html.parser').find('a').getText(),\n",
    "             elem.find('source').getText()])\n",
    "\n",
    "\n",
    "# 擷取各家新聞網站新聞函數\n",
    "def beautifulSoupNews(url):\n",
    "\n",
    "    # 設定hearers\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                             'Chrome/87.0.4280.141 Safari/537.36'}\n",
    "\n",
    "    # 取得Google跳轉頁面的新聞連結\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    newsUrl = soup.find_all('c-wiz', class_='jtabgf')[0].getText()\n",
    "    newsUrl = newsUrl.replace('Opening ', '')\n",
    "\n",
    "    # 取得該篇新聞連結內容\n",
    "    response = requests.get(newsUrl, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') \n",
    "\n",
    "    # 判斷url網域做對應文章擷取\n",
    "    domain = re.findall('https://[^/]*', newsUrl)[0].replace('https://', '')\n",
    "\n",
    "    if domain == 'udn.com':\n",
    "\n",
    "        # 聯合新聞網\n",
    "        item = soup.find_all('section', class_='article-content__editor')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "    elif domain == 'ec.ltn.com.tw':\n",
    "\n",
    "        # 自由財經\n",
    "        item = soup.find_all('div', class_='text')[0].find_all('p', class_='')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ').replace(u'\\xa0', ' '). \\\n",
    "            replace('一手掌握經濟脈動', '').replace('點我訂閱自由財經Youtube頻道', '')\n",
    "\n",
    "    elif domain in ['tw.stock.yahoo.com', 'tw.news.yahoo.com']:\n",
    "\n",
    "        # Yahoo奇摩股市\n",
    "        item = soup.find_all('div', class_='caas-body')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        del_text = soup.find_all('div', class_='caas-body')[0].find_all('a')\n",
    "        del_text = [elem.getText() for elem in del_text]\n",
    "        content = [elem for elem in content if elem not in del_text]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ').replace(u'\\xa0', ' ')\n",
    "\n",
    "    elif domain == 'money.udn.com':\n",
    "\n",
    "        # 經濟日報\n",
    "        item = soup.find_all('section', id='article_body')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = [elem for elem in content]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "    elif domain == 'www.chinatimes.com':\n",
    "\n",
    "        # 中時新聞網\n",
    "        item = soup.find_all('div', class_='article-body')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = [elem for elem in content]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "    elif domain == 'www.ctee.com.tw':\n",
    "\n",
    "        # 工商時報\n",
    "        item = soup.find_all('div', class_='entry-content clearfix single-post-content')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = [elem for elem in content]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "    elif domain == 'news.cnyes.com':\n",
    "\n",
    "        # 鉅亨網\n",
    "        item = soup.find_all('div', itemprop='articleBody')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = [elem for elem in content]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ').replace(u'\\xa0', ' ')\n",
    "\n",
    "    elif domain == 'finance.ettoday.net':\n",
    "\n",
    "        # ETtoday\n",
    "        item = soup.find_all('div', itemprop='articleBody')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = [elem for elem in content]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ').replace(u'\\xa0', ' ')\n",
    "\n",
    "    elif domain == 'fnc.ebc.net.tw':\n",
    "\n",
    "        # EBC東森財經新聞\n",
    "        content = str(soup.find_all('script')[-2]).split('ReactDOM.render(React.createElement(')[1]\n",
    "        content = content.split(',')[1].replace('{\"content\":\"', '').replace('\"})', '')\n",
    "        content = re.sub(u'\\\\\\\\u003[a-z]+', '', content)\n",
    "        content = content.replace('/p', ' ').replace('\\\\n', '')\n",
    "\n",
    "    else:\n",
    "\n",
    "        # 未知domain\n",
    "        content = 'unknow domain'\n",
    "\n",
    "    return newsUrl, content\n",
    "\n",
    "\n",
    "# 迴圈下載股票清單的Google新聞資料\n",
    "stockNews = pd.DataFrame()\n",
    "for iSearch in range(len(searchList)):\n",
    "\n",
    "    print('目前正在搜尋股票: ' + searchList[iSearch] +\n",
    "          ' 在Google的新聞清單  進度: ' + str(iSearch + 1) + ' / ' + str(len(searchList)))\n",
    "\n",
    "    # 建立搜尋網址\n",
    "    url = 'https://news.google.com/news/rss/search/section/q/' + \\\n",
    "          searchList[iSearch] + '流程自動化/?hl=zh-tw&gl=TW&ned=zh-tw_tw'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'xml')\n",
    "    item = soup.find_all('item')\n",
    "    rows = [arrangeGoogleNews(elem) for elem in item]\n",
    "\n",
    "    # 組成pandas\n",
    "    df = pd.DataFrame(data=rows, columns=['title', 'link', 'pub_date', 'description', 'source'])\n",
    "    # 新增時間戳記欄位\n",
    "    df.insert(0, 'search_time', time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), True)\n",
    "    # 新增搜尋字串\n",
    "    df.insert(1, 'search_key', searchList[iSearch], True)\n",
    "    # 篩選最近的新聞\n",
    "    df['pub_date'] = df['pub_date'].astype('datetime64[ns]')\n",
    "    df = df[df['pub_date'] >= nearStartDate]\n",
    "    # 按發布時間排序\n",
    "    df = df.sort_values(['pub_date']).reset_index(drop=True)\n",
    "\n",
    "    # 迴圈爬取新聞連結與內容\n",
    "    newsUrls = list()\n",
    "    contents = list()\n",
    "    for iLink in range(len(df['link'])):\n",
    "\n",
    "        print('目前正在下載: ' + searchList[iSearch] +\n",
    "              ' 各家新聞  進度: ' + str(iLink + 1) + ' / ' + str(len(df['link'])))\n",
    "\n",
    "        newsUrl, content = beautifulSoupNews(url=df['link'][iLink])\n",
    "        newsUrls.append(newsUrl)\n",
    "        contents.append(content)\n",
    "        time.sleep(3)\n",
    "\n",
    "    # 新增新聞連結與內容欄位\n",
    "    df['newsUrl'] = newsUrls\n",
    "    df['content'] = contents\n",
    "\n",
    "    # 儲存資料\n",
    "    stockNews = pd.concat([stockNews, df])\n",
    "\n",
    "# 輸出結果檢查\n",
    "stockNews.to_csv('checkData.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
